---
title: "P8106 Final Codes"
author: "Yuxuan Chen | Yuan Meng | Paula Wu"
output: pdf_document
header-includes:
    - \usepackage{hyperref}
    - \usepackage{cite}
---

```{r setup, echo = FALSE, warnings = FALSE, message=FALSE}
library(tidyverse)
library(ggplot2)
library(corrplot)
library(pROC)
library(caret)
library(patchwork)
library(factoextra)

knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

```{r}
bc_df = read.csv("./data/breast-cancer.csv", row.names = NULL) %>% 
  dplyr::select(-c(1,33)) %>% 
  janitor::clean_names() %>% 
  mutate(diagnosis = factor(diagnosis, level = c("B", "M")))
unique(bc_df$diagnosis)
formula_all = parse(text = paste0("diagnosis ~ ", paste(colnames(bc_df[2:31]),collapse = " + ")))[[1]]
```

```{r}
# partitioning data
set.seed(31)
indexTrain <- createDataPartition(bc_df$diagnosis, p = 0.7, list = FALSE)
trainData = bc_df[indexTrain, ]
testData = bc_df[-indexTrain,]
x = model.matrix(diagnosis~., trainData)[,-1]
y = trainData$diagnosis
```

```{r}
# very primitive EDA
bc_df_graph = 
  bc_df %>% 
  mutate(diagnosis = factor(recode(diagnosis, `1` = "M", `0` = "B"), level = c("B", "M")))
```
```{r}
cancer_mean = bc_df_graph[, 2:11] %>% as_tibble() 
colnames(cancer_mean) = gsub("_mean", "", colnames(cancer_mean))
featurePlot(x = cancer_mean,
            y = bc_df_graph$diagnosis,
            scales = list(x = list(relation = "free"),
                          y = list(relation = "free")),
            plot = "density", pch = "|",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            auto.key = list(columns = 2))

#predictor correlations
corrplot(cor(bc_df[,-1]),
         method = "circle",
         type = "upper",
         tl.cex = 0.5,
         order = "hclust")
```


## Modeling:
```{r}
ctrl <- trainControl(method = "cv",
                      summaryFunction = twoClassSummary,
                      classProbs = TRUE)
```

### Penalized Logistic Regression
```{r}

glmnGrid = expand.grid(.alpha = seq(0,1,length = 21),
                       .lambda = exp(seq(-8,-1,length =50)))
set.seed(31)        
glm_fit = train(x = x,
                y = y,
                method = "glmnet",
                tuneGrid = glmnGrid,
                metric = "ROC",
                trControl = ctrl)
glm_fit$bestTune

```

```{r}
color_set = rainbow(25)
parameter_set = list(superpose.symbol = list(col = color_set),
                     superpose.line = list(col = color_set))
glm_plot = plot(glm_fit, par.settings = parameter_set, xTrans = function(x) log(x))
```

```{r}
set.seed(31)

#vip(glm_fit)
#glm.pred.prob = predict(glm_model, 
#                        type = "response")
#glm.pred = rep("0", length(glm.pred.prob))
#glm.pred[glm.pred.prob > 0.5] = "1"
#confusionMatrix(data = factor(glm.pred, levels = c("1", "0")),
#                reference = factor(train_data$diagnosis),
#                positive = "1")
#glm.pred.prob.test = predict(glm_fit, type = "response", newdata = testData)
#roc.glm.test = roc(testData$diagnosis, glm.pred.prob.test)
#plot(roc.glm.test, legacy.axes = TRUE, print.auc = TRUE)

```

### Fit MARS 
```{r}
set.seed(31)
mars_grid = expand.grid(degree = 1:5, 
                         nprune = 2:20)

mars_fit = train(x = x, 
                  y = y,
                 method = "earth",
                 tuneGrid = mars_grid,
                 metric = "ROC",
                 trControl = ctrl)

mars_plot = ggplot(mars_fit, highlight = TRUE)
mars_fit$bestTune
coef(mars_fit$finalModel)
```


```{r}
#Training RMSE 
mars_train_se = mean(mars_fit$resample$RMSE) 
mars_train_se

#Testing RMSE 
mars_test_predict = predict(mars_fit,
                        newdata = testData)
mars_test_se = RMSE(mars_test_predict, testData$diagnosis)
mars_test_se
```

## Fit KNN

```{r}
set.seed(31)

knn_fit = train(x = x,
                y = y,
                method = "knn",
                preProcess = c("center", "scale"), 
                tuneGrid = data.frame(k = seq(1,50,by=1)),
                trControl = ctrl)
knn_fit$bestTune
knn_plot = ggplot(knn_fit,xTrans = function(x)log(x), highlight = TRUE)

#Training RMSE 
knn_train_se = mean(knn_fit$resample$RMSE) 
knn_train_se

#Testing RMSE 
knn_test_predict = predict(knn_fit,
                        newdata = testData)
knn_test_se = RMSE(knn_test_predict, testData$diagnosis)
knn_test_se
```

LDA
```{r}
# LDA
set.seed(31)
lda_fit = train(diagnosis ~. ,
                data = trainData,
                method = "lda",
                metric = "ROC",
                trControl = ctrl)
```

CART
```{r}
# classification tree
set.seed(31)
rpart_fit = train(diagnosis ~., trainData,
                  method = "rpart",
                  tuneGrid = data.frame(cp = exp(seq(-20,-2, len = 50))),
                  trControl = ctrl,
                  metric = "ROC")
rpart_plot = ggplot(rpart_fit, highlight = TRUE)
rpart_fit$bestTune
```
```{r}
rpart_pred = predict(rpart_fit, newdata = testData, type = "prob")[,2]
roc(testData$diagnosis, rpart_pred)
```

random forest
```{r}
# random forest
set.seed(31)
rf_grid = expand.grid(mtry = 1:8,
                      splitrule = "gini",
                      min.node.size = seq(from = 2, to = 10, by = 2))
rf_fit = train(diagnosis ~., trainData,
               method = "ranger",
               tuneGrid = rf_grid,
               metric = "ROC",
               trControl = ctrl)
rf_plot = ggplot(rf_fit, highlight = TRUE)
rf_fit$bestTune
```

```{r}
rf_pred = predict(rf_fit, newdata = testData, type = "prob")[,1]
roc_rf = roc(testData$diagnosis, rf_pred)
plot(roc_rf, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc_rf), col = 4, add = TRUE)
```

AdaBoost
```{r}
set.seed(31)
gbmA_grid = expand.grid(n.trees = c(2000,3000,4000,5000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.0005,0.001,0.002),
                         n.minobsinnode = 1)

gbmA_fit = train(diagnosis ~.,
                 trainData,
                  tuneGrid = gbmA_grid,
                  trControl = ctrl,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)
gbmA_fit$bestTune
gbm_plot = ggplot(gbmA_fit, highlight = TRUE)
```


## SVM (linear and radial kernel)

a) Linear Kernel

```{r}
set.seed(31)
svml_fit <- train(diagnosis~., 
                  data = trainData, 
                  method = "svmLinear2",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(cost = exp(seq(-3,2,len = 50))),
                  trControl = ctrl)
svml_plot = ggplot(svml_fit, highlight = TRUE)

svml_fit$bestTune
svml_fit$finalModel

## Linear Kernel Training Error Rate
pred_svml_train = predict(svml_fit)
train_error = mean(pred_svml_train != trainData$diagnosis)

## Linear Kernel Test Error Rate
pred_svml_test = predict(svml_fit, newdata = testData, type = "raw")
test_error = mean(pred_svml_test != testData$diagnosis)

# SVML roc:
pred_svml = predict(svml_fit, newdata = testData, type = "prob")[,1]
roc_svml = roc(testData$diagnosis, pred_svml,
               levels = c("B", "M"))

plot.roc(roc_svml, legacy.axes = TRUE, print.auc = TRUE,main = "SVML ROC Plot")
```

b) Radial Kernel
```{r}
svmr.grid <- expand.grid(C = exp(seq(-4,4,len=20)),
                         sigma = exp(seq(-4,0,len=10)))
# tunes over both cost and sigma
set.seed(31)
svmr_fit <- train(diagnosis ~ . ,
                  data = trainData,
                  method = "svmRadialSigma",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)
myCol<- rainbow(20)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))
svmr_plot = ggplot(svmr_fit, highlight = TRUE, par.settings = myPar)

svmr_fit$bestTune
svmr_fit$finalModel

# Radial Kernel training error rate
pred_svmr_train = predict(svmr_fit)
train_svmr_error = mean(pred_svmr_train != trainData$diagnosis)

# Radial Kernel test error rate
pred_svmr_test = predict(svmr_fit, newdata = testData, type = "raw")
test_svmr_error = mean(pred_svmr_test != testData$diagnosis)

# SVMR roc:
pred_svmr = predict(svmr_fit, newdata = testData, type = "prob")[,1]
roc_svmr = roc(testData$diagnosis, pred_svmr,
               levels = c("B", "M"))

plot.roc(roc_svmr, legacy.axes = TRUE, print.auc = TRUE,main = "SVMR ROC Plot")
```

##Cluster Analysis

### K-mean clustering
```{r}
index = seq.int(nrow(bc_df)) 
class = paste0(bc_df$diagnosis,"-",index)

bc_df_scale = bc_df[,2:31] %>% as.data.frame() 
rownames(bc_df_scale) = class
bc_df_scale = bc_df_scale %>% scale()

set.seed(31)
fviz_nbclust(bc_df_scale,
             FUNcluster = kmeans,
             method = "silhouette")

km <- kmeans(bc_df_scale, centers = 2, nstart = 20)
km_vis <- fviz_cluster(list(data = bc_df_scale, cluster = km$cluster), 
                       ellipse.type = "convex", 
                       geom = c("point","text"),
                       labelsize = 5, 
                       palette = "Dark2") + labs(title = "K-means") 
km_vis
```

```{r fig.height=15, fig.width=20, out.width="100%"}
library(cowplot)
plot_grid(glm_plot, knn_plot, mars_plot, rpart_plot, rf_plot, gbm_plot, svmr_plot, labels = c('Penalized Logistic  Regression', 'K-NN', 'MARS', "CART", "Random Forest", "AdaBoost", "SVM - Radial"), align = "hv", nrow = 3)
```

resampling results
```{r}
resamp = resamples(list(Plr = glm_fit,
                      MARS = mars_fit,
                      Knn = knn_fit,
                      LDA = lda_fit,
                      CART = rpart_fit,
                      RF = rf_fit,
                      GB = gbmA_fit,
                      SVML = svml_fit,
                      SVMR = svmr_fit))
bwplot(resamp)
```

variable importance
```{r}
svmr_imp = varImp(svmr_fit)$importance %>% 
  arrange(desc(M)) %>% 
  top_n(n = 20)
svmr_imp_plot = 
  ggplot(svmr_imp, aes(x = reorder(rownames(svmr_imp),M), y = M, fill = M)) + 
  geom_bar(stat="identity", position="dodge", fill = "darkseagreen") + 
  coord_flip()+
  xlab("Relative Importance") +
  ylab("Variables") +
  ggtitle("SVM Radial Kernel")

svml_imp = varImp(svml_fit)$importance %>% 
  arrange(desc(M)) %>% 
  top_n(n = 20)
svml_imp_plot = 
  ggplot(svml_imp, aes(x = reorder(rownames(svml_imp),M), y = M, fill = M)) + 
  geom_bar(stat="identity", position="dodge", fill = "darkseagreen") + 
  coord_flip()+
  xlab("Relative Importance") +
  ylab("Variables") +
  ggtitle("SVM Linear Kernel")

glm_imp = varImp(glm_fit)$importance %>% 
  arrange(desc(Overall)) %>% 
  top_n(n = 10)
glm_imp_plot = 
  ggplot(glm_imp, aes(x = reorder(rownames(glm_imp),Overall), y = Overall, fill = Overall)) + 
  geom_bar(stat="identity", position="dodge", fill = "darkseagreen") + 
  coord_flip()+
  xlab("Relative Importance") +
  ylab("Variables") +
  ggtitle("Penalized Logistic Regression")

knn_imp = varImp(knn_fit)$importance %>% 
  arrange(desc(M)) %>% 
  top_n(n = 20)
knn_imp_plot = 
  ggplot(knn_imp, aes(x = reorder(rownames(knn_imp),M), y = M, fill = M)) + 
  geom_bar(stat="identity", position="dodge", fill = "darkseagreen") + 
  coord_flip()+
  xlab("Relative Importance") +
  ylab("Variables") +
  ggtitle("K-NN")

a = plot_grid(svmr_imp_plot, svml_imp_plot, glm_imp_plot, knn_imp_plot, align = "hv")
save_plot("varimp.jpeg", a)
```

test AUC
```{r}
pred_glm = predict(glm_fit, newdata = testData, type = "prob")[,2]
pred_mars = predict(mars_fit, newdata = testData, type = "prob")[,2]
pred_knn = predict(knn_fit, newdata = testData, type = "prob")[,2]
pred_lda = predict(lda_fit, newdata = testData, type = "prob")[,2]
pred_rpart = predict(rpart_fit, newdata = testData, type = "prob")[,2]
pred_rf = predict(rf_fit, newdata = testData, type = "prob")[,2]
pred_gbmA = predict(gbmA_fit, newdata = testData, type = "prob")[,2]
pred_svml = predict(svml_fit, newdata = testData, type = "prob")[,2]
pred_svmr = predict(svmr_fit, newdata = testData, type = "prob")[,2]

roc.glm = roc(testData$diagnosis, pred_glm)
roc.mars = roc(testData$diagnosis, pred_mars)
roc.knn = roc(testData$diagnosis, pred_knn)
roc.lda = roc(testData$diagnosis, pred_lda)
roc.rpart = roc(testData$diagnosis, pred_rpart)
roc.rf = roc(testData$diagnosis, pred_rf)
roc.gbmA = roc(testData$diagnosis, pred_gbmA)
roc.svml = roc(testData$diagnosis, pred_svml)
roc.svmr = roc(testData$diagnosis, pred_svmr)

testAUC = c(roc.glm$auc[1], roc.mars$auc[1], roc.knn$auc[1], roc.lda$auc[1], roc.rpart$auc[1], roc.rf$auc[1], roc.gbmA$auc[1], roc.svml$auc[1], roc.svmr$auc[1]) %>% 
  as.tibble() %>% 
  mutate(model = c("Plr", "MARS", "KNN", "LDA", "CART", "RF", "GB", "SVML", "SVMR")) %>% 
  select(model, everything()) %>% 
  arrange(desc(value)) %>% 
  rename(., testAUC =value)

testAUC %>% knitr::kable()
```

